{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbaed1cf-100f-454d-97ee-10f20399420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers datasets torch\n",
    "#pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e715859-ef10-459c-9714-b994001935b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fasttext\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf024db5-6959-4fb6-86ef-09d5aad4cea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastTextSentiment:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        \n",
    "    def prepare_fasttext_format(self, df, output_file, text_column='review_text', label_column='sentiment'):\n",
    "        \"\"\"\n",
    "        Convert DataFrame to FastText format\n",
    "        Format: __label__positive This is a great product!\n",
    "        \"\"\"\n",
    "        print(f\"Preparing FastText format for {len(df)} samples...\")\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for _, row in df.iterrows():\n",
    "                text = str(row[text_column]).replace('\\n', ' ').strip()\n",
    "                label = f\"__label__{row[label_column]}\"\n",
    "                f.write(f\"{label} {text}\\n\")\n",
    "        print(f\"Saved FastText data to: {output_file}\")\n",
    "    \n",
    "    def train_model(self, train_file, model_save_path='sentiment_model.bin'):\n",
    "        \"\"\"Train FastText model\"\"\"\n",
    "        print(\"Training FastText model...\")\n",
    "        \n",
    "        self.model = fasttext.train_supervised(\n",
    "            input=train_file,\n",
    "            epoch=25,\n",
    "            lr=1.0,\n",
    "            wordNgrams=2,  # Uses bigrams for better context\n",
    "            dim=300,\n",
    "            loss='softmax',\n",
    "            verbose=2\n",
    "        )\n",
    "        \n",
    "        # Save the model\n",
    "        self.model.save_model(model_save_path)\n",
    "        print(f\"Model saved to: {model_save_path}\")\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def predict_sentiment(self, texts):\n",
    "        \"\"\"Predict sentiment for a list of texts\"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not trained yet!\")\n",
    "        \n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        predictions = []\n",
    "        confidences = []\n",
    "        \n",
    "        for text in texts:\n",
    "            # FastText expects clean text\n",
    "            clean_text = str(text).replace('\\n', ' ').strip()\n",
    "            preds, scores = self.model.predict(clean_text, k=1)  # k=1 for top prediction\n",
    "            \n",
    "            # Extract label and confidence\n",
    "            label = preds[0].replace('__label__', '')\n",
    "            confidence = scores[0]\n",
    "            \n",
    "            predictions.append(label)\n",
    "            confidences.append(confidence)\n",
    "        \n",
    "        return predictions, confidences\n",
    "    \n",
    "    def evaluate_model(self, test_file):\n",
    "        \"\"\"Comprehensive evaluation on test data\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"EVALUATION RESULTS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Load test data for evaluation\n",
    "        test_results = self.model.test(test_file)\n",
    "        print(f\"FastText Test Results: Precision@1: {test_results[1]:.3f}, Recall@1: {test_results[2]:.3f}\")\n",
    "        \n",
    "        # More detailed evaluation\n",
    "        true_labels = []\n",
    "        pred_labels = []\n",
    "        \n",
    "        with open(test_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                true_label = line.split()[0].replace('__label__', '')\n",
    "                text = ' '.join(line.split()[1:])\n",
    "                \n",
    "                pred_label, _ = self.predict_sentiment(text)\n",
    "                \n",
    "                true_labels.append(true_label)\n",
    "                pred_labels.append(pred_label[0])\n",
    "        \n",
    "        # Detailed metrics\n",
    "        print(f\"\\nAccuracy: {accuracy_score(true_labels, pred_labels):.3f}\")\n",
    "        print(f\"\\nClassification Report:\")\n",
    "        print(classification_report(true_labels, pred_labels, target_names=['negative', 'neutral', 'positive']))\n",
    "        \n",
    "        # Confusion matrix\n",
    "        print(f\"\\nConfusion Matrix:\")\n",
    "        print(confusion_matrix(true_labels, pred_labels, labels=['negative', 'neutral', 'positive']))\n",
    "        \n",
    "        return true_labels, pred_labels\n",
    "\n",
    "# Main execution function\n",
    "def run_fasttext_experiment(csv_file, text_col='review_text', label_col='sentiment'):\n",
    "    \"\"\"\n",
    "    Complete pipeline: Load data, train FastText, evaluate on test set\n",
    "    \"\"\"\n",
    "    # Load your dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_csv(csv_file)\n",
    "    print(f\"Dataset loaded: {len(df)} total reviews\")\n",
    "    print(f\"Label distribution:\\n{df[label_col].value_counts()}\")\n",
    "    \n",
    "    # Split data (80% train, 20% test)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[label_col])\n",
    "    \n",
    "    print(f\"Training set: {len(train_df)} samples\")\n",
    "    print(f\"Test set: {len(test_df)} samples\")\n",
    "    \n",
    "    # Initialize FastText trainer\n",
    "    ft = FastTextSentiment()\n",
    "    \n",
    "    # Prepare FastText format files\n",
    "    train_file = \"fasttext_train.txt\"\n",
    "    test_file = \"fasttext_test.txt\"\n",
    "    \n",
    "    ft.prepare_fasttext_format(train_df, train_file, text_col, label_col)\n",
    "    ft.prepare_fasttext_format(test_df, test_file, text_col, label_col)\n",
    "    \n",
    "    # Train model\n",
    "    model_path = \"sentiment_model.bin\"\n",
    "    ft.train_model(train_file, model_path)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    true_labels, pred_labels = ft.evaluate_model(test_file)\n",
    "    \n",
    "    # Test with some examples\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SAMPLE PREDICTIONS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    sample_texts = test_df[text_col].head(5).tolist()\n",
    "    sample_true = test_df[label_col].head(5).tolist()\n",
    "    \n",
    "    predictions, confidences = ft.predict_sentiment(sample_texts)\n",
    "    \n",
    "    for i, (text, true, pred, conf) in enumerate(zip(sample_texts, sample_true, predictions, confidences)):\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"Text: {text[:100]}...\")\n",
    "        print(f\"True: {true} | Predicted: {pred} | Confidence: {conf:.3f}\")\n",
    "        print(f\"✓ Correct\" if true == pred else \"✗ Wrong\")\n",
    "    \n",
    "    return ft, true_labels, pred_labels\n",
    "\n",
    "# If you want to load a pre-trained model later\n",
    "def load_and_test_model(model_path, test_csv_file, text_col='review_text', label_col='sentiment'):\n",
    "    \"\"\"Load pre-trained model and test on new data\"\"\"\n",
    "    ft = FastTextSentiment()\n",
    "    ft.model = fasttext.load_model(model_path)\n",
    "    \n",
    "    test_df = pd.read_csv(test_csv_file)\n",
    "    \n",
    "    print(f\"Testing on {len(test_df)} samples...\")\n",
    "    \n",
    "    predictions, confidences = ft.predict_sentiment(test_df[text_col].tolist())\n",
    "    true_labels = test_df[label_col].tolist()\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    print(f\"Accuracy on test data: {accuracy:.3f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(true_labels, predictions))\n",
    "    \n",
    "    return predictions, confidences, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1f1e858-4488-4fcd-b5aa-fa0ce2a2624d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset loaded: 131669 total reviews\n",
      "Label distribution:\n",
      "sentiment\n",
      "pos    87138\n",
      "neu    24704\n",
      "neg    19827\n",
      "Name: count, dtype: int64\n",
      "Training set: 105335 samples\n",
      "Test set: 26334 samples\n",
      "Preparing FastText format for 105335 samples...\n",
      "Saved FastText data to: fasttext_train.txt\n",
      "Preparing FastText format for 26334 samples...\n",
      "Saved FastText data to: fasttext_test.txt\n",
      "Training FastText model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 35M words\n",
      "Number of words:  1578209\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread:  465326 lr:  0.000000 avg.loss:  0.063214 ETA:   0h 0m 0s words/sec/thread:  496991 lr:  0.995774 avg.loss:  0.763226 ETA:   0h 9m54sm52s lr:  0.940355 avg.loss:  0.524938 ETA:   0h 8m57sh 8m53s% words/sec/thread:  515901 lr:  0.927077 avg.loss:  0.501388 ETA:   0h 8m53s 506414 lr:  0.913104 avg.loss:  0.479386 ETA:   0h 8m55s  8.8% words/sec/thread:  505969 lr:  0.911976 avg.loss:  0.476139 ETA:   0h 8m55s ETA:   0h 8m54s 504126 lr:  0.906478 avg.loss:  0.461753 ETA:   0h 8m53s words/sec/thread:  502472 lr:  0.900295 avg.loss:  0.447105 ETA:   0h 8m52s ETA:   0h 8m46s 499028 lr:  0.882821 avg.loss:  0.416308 ETA:   0h 8m45s 0.414190 ETA:   0h 8m43s 0.870309 avg.loss:  0.394961 ETA:   0h 8m35s 503202 lr:  0.862573 avg.loss:  0.379193 ETA:   0h 8m29s 507037 lr:  0.829559 avg.loss:  0.327062 ETA:   0h 8m 5s avg.loss:  0.295235 ETA:   0h 7m51s words/sec/thread:  505274 lr:  0.793475 avg.loss:  0.279620 ETA:   0h 7m46s avg.loss:  0.276803 ETA:   0h 7m45s 22.2% words/sec/thread:  502357 lr:  0.778119 avg.loss:  0.263810 ETA:   0h 7m39s39s words/sec/thread:  501876 lr:  0.776286 avg.loss:  0.261621 ETA:   0h 7m39s  0h 7m37s words/sec/thread:  502425 lr:  0.763083 avg.loss:  0.248095 ETA:   0h 7m30s  0h 7m24s 27.4% words/sec/thread:  505625 lr:  0.725806 avg.loss:  0.216572 ETA:   0h 7m 6s 28.0% words/sec/thread:  505982 lr:  0.720287 avg.loss:  0.212497 ETA:   0h 7m 2ss% words/sec/thread:  506772 lr:  0.700792 avg.loss:  0.199572 ETA:   0h 6m50s 30.1% words/sec/thread:  506810 lr:  0.699048 avg.loss:  0.198384 ETA:   0h 6m49s% words/sec/thread:  509095 lr:  0.659315 avg.loss:  0.175769 ETA:   0h 6m24s ETA:   0h 6m23s 0.171934 ETA:   0h 6m20s ETA:   0h 6m18s% words/sec/thread:  506890 lr:  0.633924 avg.loss:  0.164564 ETA:   0h 6m11s words/sec/thread:  505863 lr:  0.623848 avg.loss:  0.160199 ETA:   0h 6m 6s 0.610603 avg.loss:  0.154419 ETA:   0h 5m59s% words/sec/thread:  504002 lr:  0.603324 avg.loss:  0.151885 ETA:   0h 5m55s 503774 lr:  0.601449 avg.loss:  0.151293 ETA:   0h 5m54s 40.0% words/sec/thread:  503617 lr:  0.599857 avg.loss:  0.150833 ETA:   0h 5m53s% words/sec/thread:  503148 lr:  0.596633 avg.loss:  0.149553 ETA:   0h 5m52s 0.584604 avg.loss:  0.145286 ETA:   0h 5m45s 42.7% words/sec/thread:  501543 lr:  0.572708 avg.loss:  0.141703 ETA:   0h 5m39s 42.9% words/sec/thread:  501472 lr:  0.570723 avg.loss:  0.141028 ETA:   0h 5m37s 500149 lr:  0.547566 avg.loss:  0.133129 ETA:   0h 5m25s 46.8% words/sec/thread:  501220 lr:  0.531807 avg.loss:  0.128752 ETA:   0h 5m15s lr:  0.529470 avg.loss:  0.128136 ETA:   0h 5m13s% words/sec/thread:  501337 lr:  0.529135 avg.loss:  0.128047 ETA:   0h 5m13s avg.loss:  0.123652 ETA:   0h 5m 2s words/sec/thread:  502564 lr:  0.503447 avg.loss:  0.121597 ETA:   0h 4m57sm51s% words/sec/thread:  503709 lr:  0.478934 avg.loss:  0.116172 ETA:   0h 4m42s 0.461570 avg.loss:  0.112563 ETA:   0h 4m31s  0h 4m22s avg.loss:  0.108908 ETA:   0h 4m20s  0h 4m18s lr:  0.437913 avg.loss:  0.108065 ETA:   0h 4m17sm15s ETA:   0h 4m14s% words/sec/thread:  505537 lr:  0.429774 avg.loss:  0.106569 ETA:   0h 4m12s 505802 lr:  0.419877 avg.loss:  0.104822 ETA:   0h 4m 6s avg.loss:  0.102283 ETA:   0h 3m57s words/sec/thread:  506652 lr:  0.394347 avg.loss:  0.100520 ETA:   0h 3m51s  0h 3m43s 505401 lr:  0.376809 avg.loss:  0.097512 ETA:   0h 3m41s 0.376476 avg.loss:  0.097464 ETA:   0h 3m41s lr:  0.373535 avg.loss:  0.097087 ETA:   0h 3m39s% words/sec/thread:  505451 lr:  0.371764 avg.loss:  0.096827 ETA:   0h 3m38s 63.0% words/sec/thread:  505433 lr:  0.370418 avg.loss:  0.096637 ETA:   0h 3m37s 64.0% words/sec/thread:  505810 lr:  0.359673 avg.loss:  0.095106 ETA:   0h 3m31s% words/sec/thread:  505806 lr:  0.358988 avg.loss:  0.095009 ETA:   0h 3m30s avg.loss:  0.094903 ETA:   0h 3m30s% words/sec/thread:  506216 lr:  0.344035 avg.loss:  0.092947 ETA:   0h 3m21ss% words/sec/thread:  505178 lr:  0.326175 avg.loss:  0.090549 ETA:   0h 3m11s words/sec/thread:  504912 lr:  0.323455 avg.loss:  0.090122 ETA:   0h 3m10s 0.086832 ETA:   0h 2m56s words/sec/thread:  496959 lr:  0.290112 avg.loss:  0.085869 ETA:   0h 2m53s 72.9% words/sec/thread:  489638 lr:  0.271480 avg.loss:  0.083507 ETA:   0h 2m44s 487960 lr:  0.268285 avg.loss:  0.083213 ETA:   0h 2m43sm40s 485282 lr:  0.260835 avg.loss:  0.082733 ETA:   0h 2m39s 484082 lr:  0.256523 avg.loss:  0.082517 ETA:   0h 2m37s lr:  0.251054 avg.loss:  0.082204 ETA:   0h 2m34s 0.211425 avg.loss:  0.078809 ETA:   0h 2m12s 1s ETA:   0h 2m 0s 81.0% words/sec/thread:  471813 lr:  0.189610 avg.loss:  0.076722 ETA:   0h 1m59s 471374 lr:  0.180885 avg.loss:  0.075884 ETA:   0h 1m53s 82.3% words/sec/thread:  471172 lr:  0.176921 avg.loss:  0.075488 ETA:   0h 1m51s words/sec/thread:  471037 lr:  0.173954 avg.loss:  0.075176 ETA:   0h 1m49s% words/sec/thread:  470790 lr:  0.165415 avg.loss:  0.074575 ETA:   0h 1m44s 84.1% words/sec/thread:  470700 lr:  0.158995 avg.loss:  0.074022 ETA:   0h 1m40s 470160 lr:  0.149067 avg.loss:  0.073316 ETA:   0h 1m34s  0h 1m28s words/sec/thread:  469807 lr:  0.128604 avg.loss:  0.071784 ETA:   0h 1m21s ETA:   0h 1m20s 88.1% words/sec/thread:  469343 lr:  0.119064 avg.loss:  0.071008 ETA:   0h 1m15s  0h 1m 2s 1m 0s ETA:   0h 0m59s lr:  0.085750 avg.loss:  0.068565 ETA:   0h 0m54s ETA:   0h 0m47s 94.0% words/sec/thread:  467318 lr:  0.060390 avg.loss:  0.066972 ETA:   0h 0m38s lr:  0.047596 avg.loss:  0.066180 ETA:   0h 0m30s words/sec/thread:  466684 lr:  0.041541 avg.loss:  0.065830 ETA:   0h 0m26s 96.4% words/sec/thread:  466580 lr:  0.036200 avg.loss:  0.065517 ETA:   0h 0m23s  0h 0m19s 0.030257 avg.loss:  0.065156 ETA:   0h 0m19s words/sec/thread:  466200 lr:  0.022053 avg.loss:  0.064678 ETA:   0h 0m14s 465905 lr:  0.019185 avg.loss:  0.064513 ETA:   0h 0m12s words/sec/thread:  465828 lr:  0.012063 avg.loss:  0.064067 ETA:   0h 0m 7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: sentiment_model.bin\n",
      "\n",
      "==================================================\n",
      "EVALUATION RESULTS\n",
      "==================================================\n",
      "FastText Test Results: Precision@1: 0.812, Recall@1: 0.812\n",
      "\n",
      "Accuracy: 0.813\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.71      0.75      3965\n",
      "     neutral       0.55      0.48      0.51      4941\n",
      "    positive       0.88      0.93      0.90     17428\n",
      "\n",
      "    accuracy                           0.81     26334\n",
      "   macro avg       0.74      0.71      0.72     26334\n",
      "weighted avg       0.80      0.81      0.81     26334\n",
      "\n",
      "\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "At least one label specified must be in y_true",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m csv_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_reviews.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Run complete training and evaluation\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model, true_labels, pred_labels \u001b[38;5;241m=\u001b[39m run_fasttext_experiment(\n\u001b[1;32m      8\u001b[0m     csv_file, \n\u001b[1;32m      9\u001b[0m     text_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_text\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# change if your text column has different name\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     label_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m    \u001b[38;5;66;03m# change if your label column has different name\u001b[39;00m\n\u001b[1;32m     11\u001b[0m )\n",
      "Cell \u001b[0;32mIn[17], line 138\u001b[0m, in \u001b[0;36mrun_fasttext_experiment\u001b[0;34m(csv_file, text_col, label_col)\u001b[0m\n\u001b[1;32m    135\u001b[0m ft\u001b[38;5;241m.\u001b[39mtrain_model(train_file, model_path)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Evaluate on test set\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m true_labels, pred_labels \u001b[38;5;241m=\u001b[39m ft\u001b[38;5;241m.\u001b[39mevaluate_model(test_file)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Test with some examples\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 101\u001b[0m, in \u001b[0;36mFastTextSentiment.evaluate_model\u001b[0;34m(self, test_file)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Confusion matrix\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mConfusion Matrix:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28mprint\u001b[39m(confusion_matrix(true_labels, pred_labels, labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneutral\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m true_labels, pred_labels\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:218\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    214\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    215\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    216\u001b[0m         )\n\u001b[1;32m    217\u001b[0m     ):\n\u001b[0;32m--> 218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    228\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:481\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros((n_labels, n_labels), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39mintersect1d(y_true, labels)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 481\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one label specified must be in y_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    484\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(y_true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64)\n",
      "\u001b[0;31mValueError\u001b[0m: At least one label specified must be in y_true"
     ]
    }
   ],
   "source": [
    "# Run the complete experiment\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your actual CSV file path\n",
    "    csv_file = \"all_reviews.csv\"\n",
    "    \n",
    "    # Run complete training and evaluation\n",
    "    model, true_labels, pred_labels = run_fasttext_experiment(\n",
    "        csv_file, \n",
    "        text_col='review_text',  # change if your text column has different name\n",
    "        label_col='sentiment'    # change if your label column has different name\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
